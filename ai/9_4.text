
----------------------------------------------------------------------
09_04 인공지능을 위한 알고리즘
----------------------------------------------------------------------

[4 페이지]
우리가 인공지능을 개발하는 데 있어서 알고리즘을 이해하고 이것을 구현하는 스킬은 매우 중요합니다.

특히 딥러닝이라는 기계학습 알고리즘을 개발할 때 우리가 인간을 뛰어넘는 기계 지능이 가능해진 것은 이런 알고리즘이 조금 더 고도화되었기 때문인데요.

인공지능 알고리즘의 선조 격에 해당하는 퍼셉트론 다들 기억하십니까?
제가 중간고사 전에 이 부분에 대해서 아주 간단하게 설명 드렸는데 인공지능을 이해하는 데 있어서 매우 중요한 개념인 만큼 다시 한번 복습해 보도록 하겠습니다.
1950년대부터 인공지능에 대한 개념은 구체화되었으며 퍼셉트론의 등장으로 인공지능은 더 이상 꿈의 이야기가 아니라는 믿음을 많은 사람들이 가지게 되었습니다.
퍼셉트론은 인공신경망의 한 종류로 1957년에 코넬 항공 연구소의 Frank Rosenblatt에 의해서 처음 고안된 알고리즘인데요.
앞서 설명 드린 인간의 뉴런을 본 따 만들었기 때문에 개념적으로는 비슷한 점이 아주 많습니다.

퍼셉트론은 다수의 신호, 즉 input을 입력받아서 하나의 신호 output을 출력합니다.
뉴런이 전기신호를 내보내서 정보를 전달하는 것과 상당히 비슷한 프로세스지요. 뉴런의 수상 돌기나 축삭 돌기처럼 신호를 전달하는 역할을 퍼셉트론에서는 가중치, 즉 weight로 처리하는데 각각의 입력 신호에 부여되어서 입력 신호와의 계산을 하고 또 신호의 총합이 정해진 임계 값을 넘었을 때 1을 출력하는 방식으로 작동합니다.
각 입력 신호에는 고유한 가중치가 부여되고 가중치가 크면 클수록 해당 신호가 중요하다고 볼 수 있겠지요.
따라서 인공지능 분야에서는 가중치를 정확하게 정해서 알고리즘 또는 모델을 정교화하는 것이 매우 중요합니다.

이러한 퍼셉트론은 선형 분류 모형을 기반으로 합니다.
선형 분류라는 것은 평면에서는 딱 긋고 이 경계를 넘느냐 마느냐에 따라서 상대방을 구분하는 방식입니다.
모든 학습 데이터를 정확히 분류시킬 때까지 학습을 진행하면 학습데이터가 선형적으로 완벽하게 분리할 수 있는 수준까지 가는데요.
기계학습이 반복될수록 선의 기울기가 달라지는데 이는 학습을 하면서 가중치가 계속 조정되기 때문입니다.

화면에서 제시되는 이 그림 네 개를 보도록 하겠습니다.
왼쪽 상단부터 1번, 2번, 3번, 4번.
네 개의 그림이 그려지지요.
이 화면에 강아지와 고양이를 투입합니다.
누가 보더라도 구분이 되기 때문에 고양이와 강아지를 가로지르는 선을 하나 만들었어요.
여기에 강아지와 닮은 사물, 객체가 하나 더 들어옵니다.
그랬을 때 두 마리의 강아지와 한 마리의 고양이를 나누기 위한 선이 다시 조정이 되겠지요.

세 번째 단계에서는 여기에 강아지가 한 마리 더, 고양이가 한 마리 더 들어옵니다.
총 다섯 마리의 동물을 나눠야 되는데 강아지와 고양이를 완벽하게 구분하기 위해서 다시 기울기가 조금 더 완만해지는 선으로 구분되고 마지막으로 여기 하나가 더 투입되지요.
강아지가 한 마리 더 투입됩니다.
네 마리의 강아지와 두 마리의 고양이를 구분하기 위해서 다시 기울기가 완만하게 조정됩니다.
이런 방식으로 해서 여기에 투입되는 다양한 동물들을 완벽하게 구분할 수 있게 되는데 이렇게 해서 모델을 정교화하는 활동이 바로 가중치에 대한 조정이 되겠습니다.

하지만 환상적인 퍼셉트론의 알고리즘에 심각한 에러가 발생합니다.
바로 XOR, exclusive OR의 문제가 발견되었는데요. 우리가 네 개의 연산을 가지고 한번 표현해 보면 AND, NAND, OR, XOR은 다음과 같이 숫자들이 배치가 되며 이런 것들을 x축과 y축으로 그림을 그려보면 다음과 같이 제시됩니다.
그런데 AND, NAND, OR, 이 세 가지는 모두 직선을 이용해서 경계가 구분되지만 exclusive OR, 즉 XOR의 경우에는 직선으로 경계를 구분하지 못한다는 문제가 발견된 거지요.
즉, 이러한 방식으로는 개와 고양이를 구분하는 것이 불가능해진다는 점이 밝혀지게 된 겁니다.
사실 퍼셉트론이 인공지능 분야에서 처음 발견되었을 때 이것은 엄청난 센세이션을 불러일으켰습니다.
그리고 연구 과제도 이쪽으로 다 몰렸는데요.
exclusive OR의 사례처럼 이것이 가지는 한계점이 밝혀지면서 1차 인공지능의 붐이 끝나는 그러한 결과를 초래하게 됩니다.
이런 충격 때문이었을까요?
Rosenblatt은 1971년 7월 그의 43번째 생일에 혼자 보트를 타다가 사고로 익사했습니다.
많은 사람들은 퍼셉트론에 대한 비판으로 괴로워하던 Rosenblatt이 자살한 것이라고 말했지만 사실 이것은 누구도 모를 일이지요.

다만 분명한 건 시간이 흐른 후 그의 업적이 재조명받기 시작했고 이런 문제를 해결할 수 있는 방법이 나오는 데 그가 크게 기여했다는 것입니다.
이것이 바로 다층 퍼셉트론이 되겠습니다.
다층 퍼셉트론은 단층 퍼셉트론을 2개 이상의 층으로 쌓아서 만든 겁니다.
이를 통해서 복잡한 형태의 방정식을 표현할 수 있는 길이 열렸는데요.
즉, 인간의 뉴런과 유사한 알고리즘을 통해서 단일 퍼셉트론의 한계를 극복하는 데 다층 퍼셉트론이 혁혁한 공을 세우게 되었습니다.

화면에서 보시는 바와 같이 단일 퍼셉트론은 입력층과 출력층으로만 구성되는데요.
여기 중간에 한 층이 더 들어옵니다.
바로 이것이 Hidden Layer, 즉 히든 층이지요.
이처럼 히든 층을 하나 더 투입함으로써 컴퓨터도 인간처럼 복잡한 처리도 수행할 수 있게 된 것입니다.

지금까지 복잡한 말씀들을 드렸는데 간단하게 정리하고 넘어가도록 하겠습니다.

첫 번째, 퍼셉트론은 입력층과 출력층으로 만들어진 네트워크로 우리말로는 단층 신경망이라고 부른다. 여기에서 포인트는 뭐다? 입력층과 출력층만 있고 히든 층은 없어요.
다음으로 다층 퍼셉트론은 입력층과 출력층 사이의 히든 층이 한 개가 있다. 이것을 우리말로는 다층 신경망이라고 부릅니다.

그런데 여기서 질문, 입력층과 출력층 사이에 히든 층이 꼭 하나만 있어야 될까요? 두 개가 들어가면 안 될까요?
물론 가능합니다.
입력층과 출력층 사이에 히든 층이 두 개 이상 있는 경우가 있는데요.
우리는 그것을 일컬어 심층신경망, DNN이라고 합니다.

심층신경망이 영어로 DNN이에요.
D는 뭘까요? 심층이니까 deep일 거고 NN, 뒤에 붙은 것은 Neural Network.
그래서 심층신경망은 Deep Neural Network, 그래서 DNN이라고 부릅니다.

이처럼 퍼셉트론은 다층 퍼셉트론으로 발전했고 지금의 딥러닝 기술로 진화해 나가고 있습니다.
다음에서는 인공지능 분야에서 사용되는 대표적인 알고리즘에 대해서 아주 간단하게 설명할 텐데요.
여기에는 규칙기반 모델, SVM, k-means clustering과 같은 고전적인 알고리즘이 있고요.
또 딥러닝 관련해서는 앞서 언급한 DNN 외에도 CNN, RNN, GAN 이런 다양한 알고리즘이 있습니다.

다음에서는 각각의 개념에 대해서 제가 아주 심플하게, 최대한 쉽게 설명해 드리겠습니다.

첫 번째 규칙기반 모델입니다.
이것은 영어로 Rule-based Model이라고 하는데요.
규칙기반 모델은 사람이 하는 판단을 기계가 대신하도록 하는 알고리즘으로 If ~ then이라는 조건에 따라 대상을 분류하는 방식으로 작동합니다.
기억나시지요?
우리가 지난 7차시에 배웠던 이진 탐색 트리, 즉 Binary Search Tree는 이 규칙기반 모델에 해당하는 방법이라고 볼 수 있겠습니다.

두 번째는 SVM이에요.
이건 full로 풀어보자면 Support Vector Machine, 앞글자를 따서 SVM이라고 하는데요.
SVM은 머신러닝 기법의 하나로 서로 다른 두 그룹을 분류하기 위해서 제안된 방법입니다.
패턴을 인식하거나 자료를 분석하기 위한 지도학습 모델의 일종인 SVM은 두 카테고리 중 어느 하나에 속한 데이터의 집합이 주어졌을 때 이 알고리즘을 이용해서 주어진 데이터의 집합을 바탕으로 해서 새로운 데이터가 어느 카테고리에 속할지 미리 판단해주는 그런 역할을 합니다.
즉, 서로 다른 특징의 데이터들을 한 공간에 두면 이들 데이터를 분류하기 위해서 SVM 알고리즘이 그중 가장 큰 폭을 가진 경계를 찾아서 너희들은 이쪽이야, 너희들은 이쪽이야.
쉽게 이렇게 나눠주는 역할을 바로 SVM이 해 주고 있지요.

그림을 통해 조금 더 단순하게 설명해 보도록 하겠습니다.
우리는 화면에 있는 다양한 데이터들을 보면서 이 데이터들을 한번 분류해 본다고 가정해 보겠습니다.
그림에서 볼 수 있듯이 가운데 진한 선을 기준으로 아주 분류가 잘되고 있지요. 파란색 점들은 오른쪽에 몰려 있고 노란색 점들은 왼쪽에 몰려 있는데 이 두 개의 데이터가 서로 섞이지 않도록 굵은 선이 딱 경계를 잘 잡아주고 있습니다.
아주 예쁘게 잘 나눠줬어요.

여기서 우리가 알아야 될 개념이 두 가지가 있는데 바로 margin과 Support Vector라는 개념입니다.
가운데 진한 선으로부터 데이터 거리까지 거리를 우리가 측정해야 되는데요.
여기서 이 거리를 우리는 margin이라고 부릅니다.
그리고 결정 경계선, 즉 점선에서 가장 가까이에 있는 데이터들을 Support Vector라고 하는데요.
이 경우에 어떤 모델이 좋은 모델일까요?
당연히 margin이 커야 좋은 모델이 될 겁니다.
그런데 Support Vector는 어떨까요? Support Vector는 잘못 분류될 가능성이 매우 큰 데이터이기 때문에 이 경계가 제대로 그어졌는지 아닌지를 판단하는 데 있어서 매우 유용한 지표로 이용됩니다.

즉, margin이 크고 Support Vector가 섞이지 않도록 이 두 가지의 그룹들을 잘 분류하는 것이 가장 완벽한 경계선이 될 텐데 이런 것들을 자동으로 만들어주는 모델을 바로 SVM, Support Vector Machine이라고 부릅니다.
그런데 이런 것들은 기계로 완벽하게 만들어주는 그런 경계선은 아니에요.
중간 중간에 여러 가지의 outlier도 제거해 주고 문제가 되는 데이터들을 넣고 빼는 이런 작업들을 인간이 직접 해 주는 데 이런 정련화를 통해서 모델은 완벽해지고 이런 모델을 통해서 투입되는 새로운 사물들, 데이터를 완벽하게 분류해낼 수 있게 되는 것입니다.

군집분석은 인공지능을 구현하는 가장 대표적인 알고리즘 중 하나입니다.
각 군집에 할당된 포인트의 평균 좌표를 보고 군집의 중심점을 업데이트시키는 방식으로 알고리즘을 고도화해 나가는데요.
이런 군집분석의 종류에 k-means라는 게 있습니다. k-means의 k는 k개의 중심점을 가지고 데이터를 구분한다는 의미를 갖고 있고 means는 평균값을 가지고 구분한다고 의미를 갖고 있는데요.

즉, k-means는 k개의 중심점을 기준으로 데이터를 군집으로 묶어준다는 의미를 담고 있는 알고리즘입니다.

다음에서는 k-means clustering의 개념을 간단한 그림을 통해서 설명 드리도록 하겠습니다.

이 알고리즘을 설명하기 위해서 화면에서 여섯 가지의 그림을 하나하나 보여드릴 텐데요.
각각 단계의 이미지를 보면서 저의 설명을 들어보시기 바랍니다. 이미지 A를 보시기 바랍니다.

현재 화면에는 여러 개의 데이터가 막 흩어져 있습니다.
딱 봤을 때 여러분은 이 데이터들을 어떻게 구분하고 싶어집니까?
오른쪽 상단의 그룹과 왼쪽 상단의 그룹으로 구분하고 싶어지지요.

두 번째 단계로 넘어가서는 이러한 다양한 데이터들을 구분하기 위한 좌푯값을 한번 찍어보겠습니다.
B에서 보는 바와 같이 몇 개의 그룹으로 분류하기 위해서 저는 초기 좌표를 랜덤으로 설정했는데 여기서는 두 개의 그룹이니까 당연히 k는 2가 되겠지요.

저는 이러한 다양한 데이터들을 구분하기 위해서 초기 좌표로 빨간색과 파란색의 X 포인트를 형성해 보았습니다.
이것은 무엇을 의미하냐면 화면에 퍼져 있는 데이터들을 빨간색 X와 파란색 X에 가까운 애들끼리 다시 한번 헤쳐모여를 하겠다는 의미에요.

다음은 C 단계로 한번 넘어가 보시기 바랍니다.
그림 C에서 보는 바와 같이 초기 좌표로 형성된 빨간색과 파란색 포인트에서 가장 가까운 데이터를 한번 모아서 집결 시켜 봤어요.
그런데 여기서 포인트는 뭐냐 하면 현재 화면에서 보이는 모든 데이터에 대해서 일대일로 다 계산해 봤다는 거예요.

지금 화면에 30개가 넘는 그런 데이터들이 있는데 두 개의 임시 좌푯값으로 하나하나 계산해서 ‘나랑 가까워, 너랑 가까워?’, ‘빨간색이랑 가까워.’, ‘그러면 너는 빨간 편이야.’, ‘너랑 가까워, 나랑 가까워?’, ‘나랑 가까운 것 같아.’, ‘그럼 너는 파란 편이야.’ 이렇게 해서 화면에 있는 데이터들과 거리를 하나하나 다 비교해 봤다는 겁니다.

이런 모든 과정을 거쳐서 비슷한 거리에 있는 애들끼리 한번 모아 봤더니 이렇게 구분이 되네요.
빨강은 빨강끼리 파랑은 파랑끼리 이렇게 구분이 되었습니다.
이랬을 때 우리가 이런 생각을 해볼 것 같아요.
두 개의 랜덤 좌표를 가지고 이렇게 구분해 보기는 했는데 빨간색 데이터들과 파란색 데이터들이 경계선에 있는 것 같아.
즉, 여기서는 margin이 없는 거예요, 거의. margin이 크면 클수록 좋은 분류라고 했지요.

사실 여기에서 데이터들은 거의 비슷한 경계에 빨간색과 파란색이 같이 놓여 있습니다.
이 경우에는 완벽하게 잘 구분된, 분리된 그룹이라고 보기 어려워요. 그래서 다음 단계를 한 번 더 거치게 됩니다.

D 그림 한번 봐주시기 바랍니다.
D에서 보는 바와 같이 빨간색 그룹과 파란색 그룹의 중심점을 다시 한번 찾아서 포인트를 업데이트해 보았습니다.
확실히 빨간 선은 중앙으로 파란 선도 중앙으로 이동이 있었지요.
지금부터는 두 개의 포인트, 즉 빨간색 X와 파란색 X와 각각의 데이터들의 거리를 비교해서 가까운 애들끼리 다시 한번 재배치를 해볼 예정입니다.
그렇게 해서 보여지는 변화된 결괏값이 바로 여러분이 화면에서 보시는 이 그림입니다.
E에서 보는 것처럼 업데이트된 포인트와 가장 가까운 데이터로 다시 한번 할당시켰더니 아까와는 다른 그림이 그려졌지요.
아까는 x, y축을 봤을 때 이 선을 기준으로 위, 아래로 구분됐었는데 지금은 완전히 덩어리진 데이터 형태로 이렇게 구분이 되어 있다는 것을 그림 E에서 확인할 수 있었습니다.
그런데 이렇게 두 개의 임시 좌표를 가지고 거리를 계산했어도 이게 지금 데이터들을 정확하게 구분을 잘하는 건가, 그런 의구심이 들 수 있어요.
그래서 F에서 이 작업을 한 번 더합니다.
F에서 볼 수 있는 바와 같이 군집의 중심점이 한 번 더 업데이트되었어요.
그래서 여기 나와 있는 모든 데이터하고 거리를 다 비교했습니다.
그렇게 했는데 몇 번을 계산해도 더 이상 데이터 할당이 바뀌지 않기 때문에 이 경우는 군집이 확정되었다, 이렇게 볼 수 있는 겁니다.

앞서 그림에서는 누가 보더라도 A 그림에서는 이쪽 덩어리와 이쪽 덩어리로 구분됐었어요.
그런데 제가 좌푯값을 이렇게 찍음으로써 데이터들이 이렇게 분리가 됐었는데 이 과정을 제가 몇 번 돌리고 보니까 처음에 우리가 이렇게 구분한 게 맞다고 생각하는 대로 데이터가 잘 군집이 되었지요.

사람이 실수를 해도 컴퓨터가 이러한 고도의 알고리즘을 가지고 몇 번 구분을 해 보면 완벽한 형태로 데이터를 이렇게 군집시키는 것을 우리가 눈으로 확인할 수 있었습니다.
여러분이 보신 이러한 k-means는 머신러닝 중에 비지도 학습 기법의 일종이라는 점을 여러분이 기억하시기 바랍니다. 다음은 인공지능의 대표적인 알고리즘으로 CNN, RNN, GAN 이 부분에 대해서 다시 한번 설명 드릴 텐데요.
제가 예전 강의에서 CNN과 RNN이 ANN의 spin-off 모델이다.
즉, 진화된 모델이다.
이렇게 설명 드린 바 있었는데 혹시 기억하십니까?
CNN, RNN, GAN의 선조 격에 해당하는 ANN에 대해서 여러분이 잘 이해하고 계셔야 돼요.

ANN은 Artificial Neural Network의 약자로써 이것이 바로 인공신경망이었습니다.
이런 인공신경망에서 진화한 CNN 모델에 대해서 간단하게 살펴보면 CNN은 Convolutional Neural Network의 약자로 영어로 convolutional은 복잡한, 심오한, 이라는 뜻을 가지고 있습니다.
즉, 기존의 신경망보다 조금 더 복잡하다는 의미를 갖고 있는데요.
것은 사람의 시각 인지 과정을 모방한 심층 신경망으로 이미지를 인식하는 데 최적화된 알고리즘으로 기억하시면 되겠습니다.

두 번째 RNN, RNN은 Recurrent Neural Network의 약자인데요.
recurrent는 영어로 환류하다, 회귀하다, 라는 뜻이 있어요. 다시 거꾸로 돌아오다.
이것은 어떤 특징이 있냐면 일반적인 인공신경망과는 다르게 예전에 입력된 데이터와 현재 새롭게 입력된 데이터를 동시에 고려해서 출력값을 결정하는 방식입니다.
이것은 순차적으로 즉, 순서를 고려해야 되는 자연어 처리 분야에 폭넓게 활용되고 있는 알고리즘이다.

이것을 통해서 CNN과 RNN이 어떤 차이점을 갖고 있는지 여러분이 이해하시면 되겠습니다.

마지막으로 설명할 GAN은 Generative Adversarial Network의 앞글자를 따서 GAN이라고 부르는데요.
이것은 생성자와 구별자가 서로 대립하면서 서로의 성능을 개선시켜나가는 그런 모델입니다.
GAN은 최근에 이미지를 복원하거나 새롭게 생성하는 분야에서 탁월한 성능을 입증하고 있는 매우 hot 한 알고리즘입니다.

제가 설명을 드렸는데 아마 이해가 잘 안 되실 텐데요.
해당 모델의 기술적인 부분에 대해서는 이후 강의에서 다시 한번 자세하게 설명 드리도록 하겠습니다.
오늘 강의는 여기까지입니다.





----------------------------------------------------------------------
09_05 Outro
----------------------------------------------------------------------

[5 페이지]
인간과 인공지능의 진검승부, 가위바위보 게임으로 인공지능을 이길 수 있는 알고리즘은 무엇인가요?

2017년 서강대학교에서는 인간 대 인공지능의 가위바위보 대회가 열렸다고 합니다.
인간 팀으로는 100명의 참가자가 이 대회에 참여했고 각자의 방법으로 알고리즘을 설계한 인공지능 팀, 즉 참가자 19개 팀이 여기에 참여했는데 이 대결의 승자는 다행히도 인간이었다고 합니다.

하지만 인공지능 팀들도 여럿, 예선을 통과했는데요.
이런 인공지능 알고리즘은 어떻게 설계되었을까요?
그리고 어떠한 방법으로 인간을 이길 수 있었을까요?

인공지능은 학습을 통해 의사결정을 한다고 이미 몇 번에 걸쳐서 설명을 드렸습니다.
인간과의 내기에서 인공지능은 사전 지식이 없기 때문에 초반에는 학습이라 생각하고 가위바위보를 계속해 볼 겁니다.
그런데 가위바위보를 하면서 인공지능은 가위바위보의 패턴을 읽어냅니다. 사람들은 가위바위보를 막 내키는 대로 한다고 생각하지만 사실 잘 관찰해 보면 일정 패턴을 가지는 경우가 의외로 많습니다.

저도 한번 가위바위보를 한다고 가정해 보고 내 보겠습니다.
보, 바위, 가위, 보, 바위, 바위, 가위, 가위, 보, 바위, 바위. 두 번째 게임에서는 변칙으로 내보겠습니다. 바위, 바위, 보, 바위, 가위, 보, 바위, 바위, 가위, 보, 바위.
듣고 났더니 어때요?
상당히 랜덤으로 낸다는 느낌이 들지요.

그런데 이것을 잘 들여다보면 몇 가지의 패턴이 여기서도 확인이 됩니다.
제가 주먹을 연속으로 내는 경우가 확인이 돼요.
저는 절대로 주먹을 연속으로 내겠다고 생각을 안 했는데 왜 그럴까요? 습관인 것 같습니다.
일단 주먹으로 연속으로 내는 그런 패턴이 읽혔고요.
또 보시면 아시겠지만 보 다음에 바위가 오는 경우가 이렇게 많았습니다.
이러한 패턴을 읽고 가위바위보에 임한다면 아무래도 인공지능이 승리할 가능성이 커지겠지요.

이처럼 게임 횟수가 늘어나면 상대방의 패턴이 감지되면서 인공지능은 상대방의 패턴을 수학적 그리고 통계적 방법으로 분석해서 상대방이 가위바위보를 낼 확률을 각각 계산하고 게임에 임하게 될 겁니다.
이러한 이유로 시간이 지남에 따라 인공지능의 승률은 당연히 높아지게 되겠지요.

그렇다면 우리가 인공지능과의 게임에서 이기기 위해서 어떻게 해야 될까요?
패턴이 보이지 않도록 가위바위보를 랜덤으로 하는 방법밖에 없습니다.
이러한 방법으로 상대방에게 패턴을 읽히지 않는다면 인공지능에게 승리할 승률은 조금 더 높아지지 않을까요?

2016년 3월 13일 알파고와의 4국에서 이세돌은 백 80수 만에 백 불계승을 거뒀습니다.
알파고는 컴퓨터 스크린에 기권 메시지를 띄우며 항복을 선언했는데요.
이세돌이 알파고를 무너뜨린 백 78수는 중앙 흑 칸 사이를 띄우는 이른바 묘수였다고 합니다.
즉, 이세돌이 알파고가 생각하지 못한 그리고 한 번도 학습하지 못했던 변칙을 썼기 때문에 이겼다는 건데요.

가위바위보도 마찬가지라고 생각합니다.
인간이 가위바위보를 하는 패턴을 읽어내면 인공지능의 승리요, 그게 아니라면 팽팽한 접전이 이루어질 것입니다.

인공지능 분야에서 종사하지 않는 이상 우리가 이 모든 알고리즘을 알아야 할 필요는 없습니다.
다만 인공지능 분야의 발전은 알고리즘과 컴퓨팅, 데이터에 달려있고 각각의 기술들은 기존의 한계점들을 극복하며 지속, 발전 중에 있다는 사실만 기억하기 바랍니다.
여러분, 수고하셨습니다
