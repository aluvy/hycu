14. 웹 데이터 다루기

-------------------------------------------------------------
14_02. 인터넷 웹 기초
-------------------------------------------------------------

[2페이지]

이번 차시에는 웹 데이터 다루기에 대해서 알아보겠습니다.
이번 차시는 인터넷 웹 기초와 웹 데이터 다루기 실습으로 구성되어 있는데요.
이번 차시를 통해서 여러분은 웹 페이지 소스 코드의 구조와 웹에 들어있는 데이터의 구조를 파악하고 설명하실 수 있게 될 것입니다.
그리고 웹 데이터를 수집, 분석, 저장하는 프로그램을 코딩하실 수 있게 될 것입니다.
첫 번째 시간을 시작하겠습니다.
인터넷 웹 기초에 대해서 알아보겠습니다.

인터넷 웹의 아키텍처를 보면 바로 클라이언트 서버 아키텍처를 이루고 있습니다.
클라이언트는 흔히 우리가 사용하는 웹 브라우저 또는 웹 애플리케이션 프로그램 예를 들면 이메일 프로그램 등과 같은 것들이 이 클라이언트를 이루고 있습니다.
서버 쪽은 웹 포털이라든지 웹 페이지들을 이루고 있습니다.
이들 사이에는 HTTP 프로토콜이 동작하고 있는데 클라이언트는 서버에게 HTTP-req를 메시지로서 보냅니다.

여기에 대해서 서버는 HTTP-res 메시지로 응답하게 됩니다.
대부분의 경우 이 웹 브라우저는 웹 페이지에 있는 데이터를 request 요청을 하고, 그 요청에 대한 응답으로서 웹 페이지 또는 웹 서버는 그 데이터를 클라이언트에게 전송해주는 것입니다.
웹 서버에서 데이터를 얻고자 한다면 그 데이터에 대한 위치를 알려줄 필요가 있지요.
어떤 데이터를 원한다.
그 데이터는 어디에 있는 것이 맞다.

이런 정보를 알려주기 위해서 URL을 사용하고 있습니다.
Uniform Resource Locator의 약자지요.
URL의 용도를 정리해보면 웹 페이지나 자원의 고유한 위치를 표현하는 정보라고 볼 수 있습니다.
이런 URL의 문법 구성 요소는 문법 다이어그램 전체를 보시면 맨 왼쪽에 scheme으로부터 맨 오른쪽의 fragment까지 이루어져 있는데, 위에 있는 일직선은 하나의 필수적인 요소라고 볼 수 있고 밑에 가지를 쳐서 내려오는 것들은 일종의 옵션이라고 볼 수 있습니다.

그 문법 구성요소들로서는 체계, 허가, 경로, 질의 그리고 단편이라는 요소로서 구성이 되고 있습니다.
이중에서 scheme 즉 체계와 경로들은 바로 Primary Element라고 할 수 있지요.
필수적인 요소다.
그 나머지 허가, 질의, 단편 이런 것들은 옵션이라고 볼 수 있는 것입니다.
URL을 포함해서 이제는 HTTP 프로토콜을 통해서 메시지를 구성을 해서 서로 주고받아야 합니다.
이 HTTP 메시지는 크게 두 파트로 나누어져 있습니다.

HTTP 헤더와 애플리케이션 콘텐츠로서 구성이 되어 있고, 그중에서 HTTP의 헤더는 메시지 중에서 제어 정보를 담은 영역이라고 볼 수 있는 것입니다.
헤더의 조금 더 구체적인 예를 보시면 요청 HTTP 메시지 헤더의 예를 지금 보시고 있는데요.
GET 메시지입니다.
GET / doc /test.html HTTP/1.1이라고 하는 것은 Request Line이라고 이야기를 하고, 그 헤더의 제어부에 아주 중요한 기본적인 파트를 담당하고 있습니다.

그 밑에 있는 것들은 여러 가지 부가적인 정보를 제시하고 있는 표현하고 있는 것이지요.
이런 것들을 모두 합쳐서 Message Header라고 이야기합니다.
이런 Message Header를 구성하는 언어는 현재 보이고 있는 것은 HTML이라는 언어로서 구성된 예를 보여드리고 있는 것입니다.
응답 HTTP 메시지 헤더의 예도 마찬가지로 비슷한 구조를 가지고 있습니다.
처음에 Status Line을 가지고 있고, 두 번째로는 Response Headers 여러 가지 정보들을 구성하고 있습니다.

여기에서 Status Line을 보면 HTTP/1.1 200 OK라고 하는 세 파트의 문자열로서 구성이 되어 있습니다.
이중에서 200이라고 하는 것은 Status Code가 됩니다.
그리고 HTTP/1.1이라고 하는 것은 이 HTTP의 버전을 이야기하고 있는 것이지요.
그래서 현재 우리가 많이 사용하고 있는 HTTP의 버전은 1.1이고, 그 후에도 남아 있는 것들도 사용이 되고 있는데 여기에서 주로 1.1에 대해서 여러분에게 말씀을 드린 것입니다.
조금 전에 코드가 나왔지요.
그 코드에서 200이라고 하는 숫자는 무엇일까?

보니까 이름은 Okay고, 서버가 HTTP 요청을 성공적으로 수행해서 응답을 원하는 대로 잘 해줬다는 것을 뜻합니다.
400이라고 하는 것은 요청 자체가 문제가 있다.
잘못된 요청이라는 것이고, 403은 요청 자체는 옳지만 제대로 된 syntax라든지 제대로 요청을 한 것이지만 그 요청을 할 권한이 없거나 또는 그 요청을 들어줄 수 있는 상태가 아니다.
이럴 때는 4.3 Forbidden 거절 또는 금지의 의미로서 코드를 보내는 경우가 있습니다.

또 404라는 것은 요청 자체는 맞지만 자원을 찾지 못하겠다는 의미를 갖고 있는 것이지요.
Not Found.
이런 경우에 여러 가지 코드를 사용하는데 여기에 보여드린 것은 우리가 현재 코딩을 하면서 주로 만나게 될 수 있는 상태 코드를 선택해서 여러분에게 말씀드린 것입니다.
이 4가지 외에도 수십 개의 코드들이 사용이 되고 있는데 이런 것들은 혹시 여러분이 필요하게 되면 또 찾아보시면 금방 여러분이 이해하실 수 있을 것입니다.

우리가 보통 웹 페이지를 보면 그 웹 페이지에는 여러 이미지도 있고, 동영상도 있고, 텍스트 데이터도 있습니다.
그런 것들을 콘텐츠라고 이야기합니다.
이 웹 페이지의 전체적인 구조, 프레임 속에 이런 콘텐츠들이 일정한 포맷을 따라서 정해진 포맷을 따라서 표현이 되고 있는 것이지요.
그런 콘텐츠를 표현하는 것.
또는 프레임을 구성하는 것을 어떤 것으로 하는가.

그때 바로 웹 페이지를 프로그래밍을 한다고 하고, 그 프로그래밍을 하는 언어가 HTML이 되는 것이고, 여기에 부가적으로 CSS, Cascading Style Sheet 등의 언어로 이 웹 페이지를 코딩하는 것입니다.
물론 이 CSS뿐만 아니고, 자바 스크립트도 사용해야 될 것이고 여러 가지 웹 스크립트들도 사용해야 되지만 우리는 간단하게 여기에서는 HTML 그리고 조금 더 나아간다면 CSS 정도만 생각하도록 하겠습니다.

이 HTML은 CSS 등의 언어와 함께 브라우저가 표시하는 콘텐츠의 크기, 위치, 폰트, 레이아웃 이런 표현 방식을 지정하는 데 사용이 되는 것입니다.
웹 페이지의 예를 한번 보겠습니다.
자세히 보시면 이 웹 페이지의 URL 주소가 나타나 있는데 그 주소를 여러분이 그대로 여러분의 브라우저에서 입력하고 불러보시면 이와 동일한 페이지가 나타날 것입니다.
단, 단서는 당분간입니다.

웹 페이지라고 하는 것은 수시로 바뀔 수 있기 때문에 현재 제가 여러분에게 말씀드리는 시점에서 보여지는 이 사이트의 모양이 언제 또 바뀔지 모르는 것이기 때문에 꼭 이와 같은 똑같은 모습이라고 여러분에게 말씀드릴 수는 없겠고, 다만 이런 웹 사이트들이 바로 지금 제가 말씀드렸던 웹 페이지라는 것을 보여드린 것입니다.
이 웹 페이지의 소스 코드의 예를 보면 처음에 HTML이라고 하는 태그가 나옵니다.

그리고 이 HTML이 어떤 언어로 텍스트로서 작성이 됐는가.
바로 영어로 작성되어 있다는 부가적인 정보까지 보여주고 있습니다.
그러면 박스의 맨 밑을 보시면 /html이 나오지요.
그래서 html과 /html 이 사이에 있는 것들이 바로 HTML 언어로서 작성된 코드들이다.
코드 리스트라는 것을 알려주고 있는 것입니다.
그다음 위에서 두 번째 줄에는 head가 나오고 있습니다.

이것은 이 페이지의 헤드 파트를 구성하는 코딩 부분이라는 것이고, 그다음에 body가 나오고 있습니다.
그 body는 바로 이 페이지의 주요 중심 내용을 구성하고 있는 코딩 부분이라는 것을 의미하고 있는 것이지요.
그 body 속에는 구조에 따라서 여러 가지 태그들이 또 나오게 되는데 헤더가 또 나오게 됩니다.
여기에 타이틀을 주고 싶으면 h1 타입의 타이틀을 주어서 조금 더 굵고 큰 글씨로 Alan Simpson이라고 하는 사람의 이름을 페이지에 표시를 할 수 있도록 하겠다는 것이지요.

또 img라고 하는 것은 이미지.
이미지를 표시하는데 그 이름은 hamburger이고, 소스 이미지가 있는 경로는 소스는 assets/images 그다음에 hamburger.png라는 파일 속에 들어있다는 것입니다.
hamburger는 무엇이지요?
여러분 이 페이지에서 보시면 맨 오른쪽 위에 있는 가로로 3개의 줄이 가 있는 것이 있습니다.
상등병, 육군의 상등병 계급장처럼 표시라고 우리는 이야기할 수 있지만 구미 쪽 사람들은 유럽이나 미국 쪽 사람들은 햄버거처럼 생겼다고 해서 햄버거라고 부르고 있습니다.

이런 아이콘이 어디에 있다.
이름은 무엇이라는 것을 이 코딩에서 보여주고 있는 것입니다.
그다음에 article 파트가 있습니다.
article 파트 속에는 p 태그로서 이루어진 것 또는 a 태그로서 이루어진 것들이 있는데 전부 이 웹 페이지에 들어있는 콘텐츠들의 URL이라든지 또는 URI에 대해서 표현해주고 있는 부분들입니다.

이런 코딩 내용만 본다면 어떤 자원들이 이 웹 페이지에 지금 표시가 될 것이고, 실제로 이 자원들은 이미지라든지 텍스트라든지 동영상이라든지 하는 자원들은 이 웹 사이트를 지원하는 호스트의 어떤 디렉터리에 들어있는지를 파악할 수 있는 것이지요.
직접 액세스해서 또 가져올 수도 있지만 사실은 그렇게 할 수는 없고, 항상 웹 서버를 통해서 우리는 받을 수밖에 없는데 브라우저는 다 이런 코딩을 이해하고 있는 것이지요.
그럼으로써 실제 이 이미지를 받아오면서 원하는 코딩 된 부분에서 원하는 대로 표시를 해줄 수 있는 것입니다.

그러면 그 브라우저와 같은 기능을 하는 것을 이제 우리는 파이썬으로서 또 만들어보겠다는 것입니다.
브라우저처럼 화면에 표시하는 것이 아니고, 이제는 우리가 원하는 데이터만 추출을 해서 잘 정리를 해서 파일로서 저장하고 나중에 사용할 수 있도록 하겠다는 것이 조금 먼저 말씀드리는 이번 차시의 목적이 되는 것입니다.
코딩된 웹 페이지가 우리의 브라우저에서 어떻게 나타나는지 그림으로 보겠습니다.

바로 이 그림이 여러분이 흔히 브라우저를 통해서 보실 수 있는 웹 페이지의 화면이 되는 것이지요.
이 화면은 html 소스 코드로 구성이 되어 있는데요.
여러분이 보시는 이 html 소스 코드는 전체는 아닙니다.
일부만 보여드리는 것이지요.
중간에 있는 것들은 다 중요하지 않은 것들은 다 솎아내고 그 구조를 보여드리기 위해서 중간 중간을 생략한 상태에서 보여드리는 것입니다.

이 소스 코드에 대해서 말씀드리면 페이지는 태그 body와 body 사이에 코딩이 되고 있습니다.
그리고 페이지의 주요 내용은 대개 article과 /article 사이에 코딩이 되고 있고, 콘텐츠의 저장 디렉터리는 anchor 태그인 a와 /a 사이에서 href 속성으로 코딩이 됩니다.
이미지 콘텐츠 저장 디렉터리는 태그 img의 소스=속성으로서 코딩이 되고 또 이 이미지라든지 여러 콘텐츠들에 대한 부가적인 설명 또는 캡션이 있는데 그 캡션을 표시할 수 있는 텍스트 자원들은 span과 /span 사이에 있는 링크 텍스트로서 표현이 되는 것입니다.

이런 소스들을 보시면 이 웹 페이지에 들어있는 콘텐츠가 무엇이라는 것을 우리도 이해할 수 있고, 우리가 사용하는 웹 브라우저도 이해할 수 있는 것입니다.
조금 더 나아간다면 앞으로 우리가 개발할 파이썬 프로그램도 이런 소스 코드를 보면서 어떤 자원이 어디에 들어있다는 것을 이해할 수 있는 것이지요.
그런 정보를 또 따로 추출해서 파일로 저장할 수도 있는 것입니다.

그렇게 하려면 이제 우리의 프로그램이 이 웹 페이지 내용을 분석할 수 있어야 되겠지요.
그 분석을 할 수 있는 몇 가지 도구들이 있습니다.
중요한 도구들이 있는데 첫 번째는 파이썬에서 제공하는 패키지인 urllib라는 패키지입니다.
패키지라는 것은 여러 모듈들이 모여 있는 하나의 디렉터리를 의미한다고 보셔도 됩니다.
그리고 하나하나의 모듈들은 .py 파일이라고 말씀드린 적이 있지요.
이런 구조를 가진 패키지로서의 urllib를 우리가 사용하면 웹 페이지 내용을 분석할 수 있는 것입니다.

간단하게 말씀드리면 인터넷 URL을 다루는데 사용하는 여러 가지 모듈들을 모아놓은 패키지인데 어떤 모듈이 있는가 밑에 표를 보시면 request는 URL에 대한 연결을 설정하는데 사용하는 모듈이고, response는 서버의 응답을 처리하는 모듈이고, error는 Request에서 발생하는 예외를 처리하는 모듈이 됩니다.
parse는 URL의 구성요소의 속성을 해석하는, 분석을 하는 기능을 제공하는 모듈이 됩니다.

또 하나 부가적인 것이기는 한데 robotparser라는 것이 있습니다.
웹 사이트의 robots.txt 파일의 내용을 분석하여서 웹 응용 프로그램의 각 자원에 대한 엑세스 가능 여부를 판별할 수 있도록 하는 기능을 가진 모듈이 되는 것입니다.
예를 들어서 여러분은 Google에 robots.txt 파일을 찾아보실 수 있습니다.
그리고 또 하나는 클래스로서 제공되는 BeautifulSoup가 있습니다.
BeautifulSoup라고 하는데 그 이름은 어디에서 나왔는가.

이것은 찾아보니까 이상한 나라의 앨리스에서 나오는 이름이라고 하는데 무엇인가 신기한 마법의 수프 같다고 해서 그런 역할을 하는 클래스라고 해서 이름을 BeautifulSoup라고 지은 것 같습니다.
간단하게 보면 패키지 BS4에 정의가 되어 있습니다.
BS4라는 것도 역시 BeautifulSoup의 머리글자들을 따서 BS라고 할 수 있는 것이지요.
그 버전이 4라는 뜻입니다.

웹 페이지를 구성한 HTML, CSS 또는 JS 자바 스크립트 등의 언어를 구문 해석하는 기능을 제공하는 클래스가 되는 것입니다.
이런 클래스의 정의 구문 문법을 보면 class BeautifulSoup(markup=nullstring, features=None, builder=None, parse_only=None, from_encoding=None 등등으로 여러 가지 디폴트 옵션을 가진 default value를 가진 optional한 파라미터들을 가지면서 이 클래스가 정의가 되고 있습니다.

이중에서 markup은 빼놓을 수 없는 optional은 아니고, 우리가 항상 주어야 되는 파라미터인데 바로 해석이 되어야 하는 해석 대상 웹 페이지의 소스 코드 리스트를 담고 있는 구조체가 되는 것입니다.
또 파라미터 features는 페이지 해석에 필요한 해석기 parser의 기능과 라이브러리를 명시해주고 있습니다.
xml이라든지 또는 html 또는 html5 등에 대한 파징을 하고 있다는 것을 정보를 제공하는 것이 이 features가 되는 것입니다.

이제는 조금 전에 말씀드렸던 urllib 그리고 클래스인 BeautifulSoup 이 두 도구를 가지고 웹 페이지 내용을 분석해 들어가는 과정을 살펴보도록 하겠습니다.
페이지 분석 절차를 보겠다는 것이지요.
첫 번째로 해야 될 일이 바로 웹 페이지 소스 코드를 다운로드하는 것입니다.
먼저 웹 페이지 웹 서버에 접속을 해서 URL을 보내면 거기에 해당하는 response를 주고, 그 URL에 요청되어 있는 페이지 html 파일을 서버가 주게 되지요.
그 과정은 바로 그 밑에 보시는 박스에 들어있는 소스 코드 다운로드 구문 문법으로 실행이 되는 것입니다.

왼쪽에 보시면 page-src-obj가 있고 =urllib.request.urlopen이라는 메소드의 URL이라는 인수를 주어서 호출을 하면 URL에 해당하는 사이트에 접속이 되고 또 URL 속에 있는 URI에 해당하는 자원이 다운로드가 되는 것입니다.
그 자원은 처음에는 웹의 포털 사이트 홈페이지가 되는 메인 페이지가 되는데 그 index.html이라든지 또는 그에 준하는 내용을 가진 HTML 파일이 다운로드가 되는 것입니다.
그리고 객체 참조에서 page-src-obj가 가지고 있는 것이 바로 이 HTML 소스 코드를 다 가지고 있는 객체에 대한 참조를 가지고 있는 것이지요.

그다음에는 이 다운로드 받은 소스 코드를 파징을 해야 되는 절차가 기다리고 있네요.
이를 위해서 먼저 해야 될 일은 바로 이 parser 즉 해석기에 해당하는 객체를 생성해줘야 됩니다.
그 parser 객체는 BeautifulSoup 클래스로부터 생성이 되는 것이지요.
밑에 구문을 보시겠습니다.
soup-obj라는 parser 객체는 BeautifulSoup라는 이 클래스의 조금 전에 다운로드 받은 소스 코드를 가지고 있는 객체인 page-src-obj를 인수로 주고, 그다음에 bs-feacher 등을 인수로 줍니다.

여기에서 bs-feacher라는 것은 앞에서도 말씀드렸듯이 html이라든지 또는 html5라든지 이런 소스 코드를 구성한 언어가 무엇인지를 지정을 해주는 것이 두 번째 인수로 제공이 되는 것입니다.
나머지는 그렇게 큰 신경을 쓸 필요는 없습니다.
그리고 기본적인 디폴트 값을 그대로 갖고 생성이 되도록 하면 이 soup-obj라는 parser가 생성이 되는 것입니다.
이런 parser를 이용해서 페이지에서 우리가 주로 분석을 해야 되는 부분의 소스 코드를 추출하는 과정이 세 번째 단계가 되겠습니다.

그 과정을 박스 안에 표현을 해놓았습니다.
구문으로서 표현해놓았는데 조금 전에 soup-obj.article이라는 이 attribute를 article 코드라는 객체로서 배정을 받는 것입니다.
바로 이렇게 하면 .article 속에는 무엇이 들어있느냐 하면 article, /article이라는 그 두 태그 사이에 있는 콘텐츠와 관련도가 높은 소스 코드가 그대로 이 article 코드에 배정이 되는 것입니다.

그러고 나서 소스 코드를 분석해서 이 소스 코드에 지정되어 있는 콘텐츠에 대한 정보를 추출하는 단계가 네 번째 단계가 됩니다.
추출 대상은 각 콘텐츠에 대한 링크에서 URL이라든지 이미지 파일 경로라든지 텍스트 등의 콘텐츠 정보이고, 이들을 dict 유형 객체로서 표현을 하는 것입니다.
콘텐츠 링크의 예를 보면 html로서 어떻게 작성이 됐는지 보면 a 태그가 열리고, href=https 등으로 시작하는 URL이 주어지지요.

그러고 나서 그다음에 바로 img record가 시작이 됩니다.
여기에서는 소스 attribute의 jpg 이미지가 들어있는 디렉터리에 URI를 배정해주는 것이지요.
그러고 나서 그 밑에는 span으로서 Lists라는 하나의 설명문을 주는 구조로서 이 콘텐츠에 대한 링크 구조가 형성이 되어 있습니다.
이런 것들을 이 Parser는 이해하고 여기에서 필요한 정보를 각각 분리를 해서 추출을 해내는 것이지요.
어떻게 추출을 하는지 보겠습니다.

이러한 콘텐츠 링크가 하나만 있는 것은 아니지요.
anchor record가 태그 a로서 구성이 되어 있는 anchor record는 여러 개가 구성이 되어 있기 때문에 대부분의 경우는 for 루프 구문을 주로 사용하게 됩니다.
콘텐츠 링크가 여러 개이기 때문에 for 구문으로서 콘텐츠 정보를 순차적으로 추출하여서 Dict 유형 객체를 항목으로 가진 list 유형 객체로 표현합니다.
말이 조금 복잡하기는 한데, 제가 드리는 말 그대로를 하나하나 문자적으로 따라가 보면 이 밑에 있는 소스 코드와 비교하면서 보시면 이해를 또 하실 수 있을 것입니다.

이런 내용의 추출 구문의 문법을 보면 먼저 contents-list라고 하는 리스트 객체를 하나 생성합니다.
여기에서 이름은 무엇으로 주어질지 모르고 여러분의 자유에 맡기기 위해서 변수로서 처리를 했습니다.
BNF 변수로서 처리를 했습니다.
그다음에 for 키워드를 주고, content-link in soup-obj.find_all 그다음에 find_all이라고 하는 메소드 안에는 인수로서 a라는 문자를 주었습니다.

바로 a 태그가 형성한 a와 /a 사이에 형성되어 있는 anchor record를 모두 찾아내서 iterable하게 만들어놓고 그것을 한 번씩 한 번씩 for 루프를 통해서 처리를 하라는 구문이 되는 것입니다.
하나의 a record는 바로 콘텐츠 링크라는 객체에 담겨서 for suite 안으로 들어오게 되겠지요.
그 콘텐츠 링크에서 get를 하면 되는데 그 get에서 href라는 키워드를 가진 부분을 찾는 것입니다.
찾아서 뽑아내면 그것이 url이 되는 것이고, 그 url은 바로 url이라고 이름이 붙여진 변수에 배정이 되는 것입니다.

img도 마찬가지입니다.
content-link.img.get 메소드를 사용하면 되고 여기에 소스를 attribute로서 찾으면 되는 것입니다.
span도 마찬가지입니다.
span에서 .text라는 attribute를 찾아서 넣으면 그 콘텐츠에 들어있는 캡션 텍스트가 나오게 되고 그것을 span이라고 이름 붙여진 변수에 assign이 되는 것입니다.
그리고 이렇게 찾아낸 Value들을 URL이라든지 img라든지 span을 전부 3개의 항목을 가진 dictionary로 만든 것이 content-dict입니다.
content key, img key, img key지요.

그다음에 span key라고 하는 임의의 이름으로 지정할 수 있는 그런 변수로서 key를 삼고 조금 전에 찾았던 URL, img 그다음에 span이라고 하는 value를 각각 배정을 해서 3개 항목을 만든 것입니다.
그것을 이 content-dict를 content-list에 append를 시킨 것이지요.
그렇게 함으로써 앞에서 말씀드린 대로 Dict 유형 객체를 항목으로 가진 list 유형 객체로서 표현을 한다는 것입니다.
그다음에 마지막 단계입니다.

자, 지금까지 한 것은 우리가 원하는 웹 사이트에 접속해서 그 웹 사이트의 홈페이지로부터 홈페이지를 구성하는 html 소스 코드를 다운로드 받고 그다음에 parser를 통해서 우리가 원하는 콘텐츠가 집중적으로 들어있는 부분을 따로 떼어내서 article part지요.

article part인데 그 article part를 떼어내서 그 속에 또 들어있는 anchor record를 하나하나씩 for 루프를 돌리면서 anchor record 속에 있는 URL과 img와 span 파트를 각각 추출을 해서 이것들을 딕셔너리로 만들고 그 딕셔너리를 다시 여러 개가 나올 것이니까 list로 만들었다는 것이 지금까지의 과정이고, 이제 이런 것들을 이용해서 우리가 원하는 포맷으로 앞으로 다른 응용 프로그램에 이용할 수 있는 형태로 파일로 저장을 해보자는 것이 마지막 단계가 된 것입니다.

여기에 사용되는 파일의 포맷은 대표적으로 사용되는 CSV 파일 포맷입니다.
즉 Comma-Separated Values라는 형태 파일이 되는 것이지요.
이 파일을 다루기 위해서 모듈 CSV를 또 도입을 할 필요가 있습니다.
import csv하면 그 작업은 끝나게 됩니다.
그러고 나서 CSV로서 저장을 할 파일을 열어야 됩니다.
라이트 모드로서 파일을 열어야 되지요.
with 구문을 사용했는데 open을 사용했습니다.

그리고 CSV 파일 네임을 주고 라이트 모드를 주고 newline은 nullstring으로서 주었습니다.
as csvfile-obj 콜론이라는 형태로 구성된 with 구문을 사용했습니다.
그리고 이때 newline에 nullstring을 제공하는 것은 newline이 자동적으로 붙는 것을 방지하기 위해서 준 것이고, 이렇게 주어야지 compact한 CSV 파일이 만들어지게 되는 것입니다.
파일은 열었습니다.

그러면 파일에 우리가 가지고 있는 추출한 소스 코드의 일부, 일부들을 그 정보들을 파일의 CSV 형태로 저장을 해야 되겠지요.
그 CSV 포맷으로 저장하는 기능을 제공하는 클래스가 바로 _csv.writer라는 클래스이고, 그 클래스를 바로 우리가 생성할 수는 없고 _csv.writer라는 함수를 통해서 이 writer 객체를 생성합니다.
그 구문 형태는 밑에 박스에 표현이 되어 있습니다.

_csv.writer라는 함수에 조금 전에 오픈을 했던 csvfile-obj를 인수로 주어서 호출을 하면 리턴되는 값은 _csv.writer object가 나오는 것입니다.
이런 _csv.writer object는 CSV 포맷 텍스트 데이터를 파일에 저장하는 용도로 사용이 되는 것입니다.
처음에 우리가 저장을 할 것은 바로 헤더 타이틀 행입니다.
한 줄의 헤더 타이틀 행을 작성하는 데에는 바로 writer-obj.writerow( )라는 메소드를 사용합니다.

이때 writerow( )의 인수는 title-문자열의 list를 제공하면 되는 것입니다.
대괄호 열고 title-string, title-string, 2개면 2개 하나면 하나 또는 nulllist를 줘도 됩니다.
그런 nulllist를 주면 헤더가 전혀 표시가 되지는 않겠지요.
그냥 빈 줄이 하나 나타나겠지요.
그런 줄도 가능합니다.
그렇게 헤더 타이틀 행을 작성할 수 있습니다.
그다음에는 실제 내용이 들어가는 것이 그다음 행으로서 형성이 되겠지요.

콘텐츠 링크가 하나만 있으면 한 줄짜리 행만 생기면 되고, 4개가 있으면 4개의 행이 생기면 되지요.
이런 역할을 하는 것이 for 구문이 됩니다.
links-list에서 포함되어 있는 하나하나의 row를 꺼내서 writer object를 통해서 이 CSV 파일에 저장을 시키는 것입니다.
그 구조가 밑에 박스에 나와 있는 것이지요.
이때 links-list는 콘텐츠 추출 과정에서 앞에서 우리가 설명을 보았던 그 contents-list라는 것과 같다고 보시면 되겠고요.

row라는 것은 그 contents-list의 항목인 유형 dict 객체인 content-dict라고 보시면 되겠습니다.
자, 이렇게 해서 저장을 하면 하나의 CSV 파일이 형성이 되고, 그 CSV 파일을 흔히 엑셀을 통해서 다시 불러서 볼 수 있고, 적당한 형태의 데이터로서 가공을 할 수 있고 여러 가지 값들을 추출을 또 할 수 있는 것입니다.

이런 과정을 통해서 우리는 하나의 웹 사이트로부터 우리가 원하는 콘텐츠가 들어있는 여러 정보들을 꺼내서 콘텐츠 자체는 아닙니다.
이미지라든지 그것은 나중 이야기고, 우선은 지금까지 우리가 거쳤던 모든 과정에서 얻은 것은 콘텐츠를 지정을 하는 URL이라든지 URI라든지 또는 이미지에 대한 패드라든지 즉 디렉터리 패드지요.
또는 URI지요.

그다음에 여러 가지 설명문들 텍스트들을 꺼내서 저장을 할 수 있다.
그 저장된 다음에는 여러 가지 응용 프로그램을 또 이용해서 우리가 원하는 형태의 정보를 꺼낸다든지 원하는 형태로 가공을 해서 표시를 할 수 있다는 것을 말씀을 지금까지 드린 것입니다.
그러면 조금 이따가 실습을 통해서 실제 우리의 코드가 어떻게 구성이 되고 또 어떻게 동작하는지 보도록 하겠습니다.
