-------------------------------------------------------------
14_03. 웹 데이터 다루기 실습
-------------------------------------------------------------

[3페이지]

이제 강의에서 말씀드렸던 내용에 대해서 실습으로 확인을 해보겠습니다.
먼저 오늘은 2가지 환경을 사용할 것입니다.
첫 번째로는 비주얼 스튜디오 코드를 사용하고 또 나머지 부분에 대해서는 주피터 노트북을 사용하도록 하겠습니다.


**********************************실습영상***********************************
우리가 웹 페이지를 연다는 것은 어떤 것이 될까요?
브라우저를 한번 실행시켜보겠습니다.
이 브라우저를 통해서 임의의 웹 페이지를 펼쳐보겠는데, 그 웹 페이지는 가급적이면 우리가 샘플로 삼고 있는 웹 페이지를 보도록 하겠습니다.
여러분의 강의 노트에도 있는 이 URL을 복사를 해서 보면 아주 단순한 웹 페이지가 열렸습니다.
Sample Content라고 하는 헤더 타이틀과 작게 보이는 텍스트들이 있지요.

무슨 뜻인지는 한번 보시기 바랍니다.
이 웹 페이지에 들어있는 소스 코드는 무엇인지 보겠습니다.
오른 클릭을 해서 지금 제가 사용하고 있는 브라우저는 크롬입니다.
이 크롬의 오른 클릭 메뉴에서 페이지 소스 보기를 보면 단순하지만 이런 소스 코드를 볼 수 있습니다.
html로 시작했습니다.
head에 있고, head에서 여기 head에는 이 페이지를 구성하는 여러 가지 정보들이 담겨 있지요.

그리고 여러분이 보셨던 Sample Content가 head1 형태로 표시가 되고, 그 밑에 바로 텍스트가 나오도록 되어 있습니다.
이제 우리가 만들어가고자 하는 프로그램은 바로 이 소스 코드를 이와 똑같은 것은 아닙니다.
다른 사이트인데 소스 코드를 다운로드 받아서 이 소스 코드 중에 있는 콘텐츠에 대한 정보를 추출해서 따로 저장하는 과정까지 보도록 하고자 하는 것입니다.
우선 비주얼 스튜디오 코드를 Launch를 시키겠습니다.


Workspace를 만들고, 코딩을 할 수 있는 에디터 창을 여는 과정은 이미 여러분과 수차례 반복을 해왔던 작업이기 때문에 생략을 하고 그 결과 화면부터 보시도록 하겠습니다.
비주얼 스튜디오 코드가 열렸습니다.
이 상태에서 새로운 소스 파일을 만들도록 하겠습니다.
열려라 URL이라는 이름으로 파일 이름을 만들었습니다.
코딩에 들어가겠습니다.
자, 코딩이 끝났습니다.
함께 보시겠습니다.

먼저 웹 페이지에 접속하기 위해서는 웹 사이트에 접속하기 위해서 필요한 것이 urllib라고 하는 패키지에서 많은 모듈들이 필요하지요.
그중에서 대표적인 것이 request입니다.
그래서 urllib로부터 request라는 모듈을 import하자.
그다음에 URL은 무엇으로 주었느냐 하면 샘플 URL은 조금 전에 보셨던 그 사이트입니다.
AlanSimpson.me/python/sample.html이라는 URL을 주었습니다.

그 URL을 가지고 request.urlopen(sample_url)로서 그 웹 사이트를 열고, 연 다음에 무엇이 넘어온다고 했지요?
그 웹 사이트의 홈페이지를 구성하는 소스 코드가 다운로드가 된다고 했지요.
그 다운로드 되는 소스 코드를 웹 페이지라는 객체로서 배정을 받은 것입니다.
그다음에 그 작업에 대한 결과를 확인하기 위해서 status를 확인했고 또 우리가 다운로드 받은 웹 페이지가 어떤 유형의 객체인지를 확인하기 위해서 타입을 확인하도록 했고 또 내용을 읽어보기 위해서 read 메소드를 실행시킨 것입니다.

그리고 프로그램이 완전히 끝났음을 표시하는 텍스트를 한 줄 print하도록 했습니다.
실행을 시켜보겠습니다.
이 파일에서 오른 클릭을 해서 메뉴를 열고 Run Python File in Terminal이라는 영역으로 실행을 하겠습니다.
실행이 됐습니다.
처음에 status는 200이 됐습니다.
Okay지요.
여기에 4번 줄에서 실행된 결과가 200으로 표시가 된 것이지요.
그다음에 type은 5번 줄 구문이 실행된 결과입니다.

이 type.
웹 페이지라는 다운로드 된 html 소스 코드의 type은 무엇인가?
바로 http.client.HTTPResponse라고 하는 메시지를 담은 형태의 유형을 뜻하는 것입니다.
그리고 dump를 했었지요.
웹 페이지의 내용을 read 메소드를 통해서 dump를 했습니다.
dump를 했더니 처음에 여기에 나오는 것은 이 b라는 것은 Byte라고 하는 것입니다.

텍스트는 아니고, 우리가 읽었을 때는 이게 텍스트 모양으로 나타나기는 하지만 지금 Python은 이것을 텍스트로 처리하는 것이 아니라 텍스트로서의 의미를 갖지 않은 단순한 바이너리 데이터로 해석해서 표시를 해준 것뿐입니다.

그렇기 때문에 여기에 백 슬래시 n이라든지 하는 newline 같은 것들이 그대로 고스란히 우리 눈에 보이도록 표시가 된 것이지요.
그러면 이 내용 DOCTYPE html 또 html lang는 en 등등의 이런 내용들이 바로 다운로드 받은 HTML 소스 코드의 내용이 되는데 그 내용이 조금 전에 여러분과 함께 보았던 웹 페이지의 샘플 html 코드와 같음을 확인할 수 있습니다.

*************************************끝**************************************

이렇게 이번에 실습을 한 부분은 단지 urllib 속에 있는 리퀘스트 모듈을 통해서 웹 사이트에 접속하고 요청한 대로 그 홈페이지에 있는 소스 코드를 다운로드 받는 과정까지를 실습해본 것입니다.
그다음에 두 번째로는 이게 웹 접속이 항상 성공하는 것은 아닙니다.
실패를 하는 경우도 있습니다.
실패를 하는 경우는 여러 가지가 있는데 URL을 잘못 주었기 때문에 실패하는 경우도 있겠지만 공인된 또는 인정된 응용 프로그램이 아니면 접속을 거부하는 사이트도 있기 때문에 우리가 만든 파이썬 프로그램이 접속을 하지 못하는 경우도 생길 수 있습니다.


**********************************실습영상***********************************
새롭게 탭을 열어서 확인해보겠습니다.
구글에서 어떤 로봇 정책을 갖고 있기 때문에 그럴지 보겠다는 것입니다.
보통 로봇 채점 TXT 파일은 그 사이트의 가장 위쪽에 있는 루트 디렉터리에 있습니다.
com의 robots.txt를 엔터를 하면 이와 같이 robots.txt의 내용이 표시가 됩니다.

*************************************끝**************************************

이중에서 Disallow: /search가 되어 있습니다.
이것은 어떤 에이전트든지 여기에 직접 들어오는 금지한다는 것입니다.
이렇게 금지시켜놓고 이제 밑에 가서는 무조건 금지 시켜놓고 밑에 가서는 또 접근을 허용할 것은 세부적으로 직접 지정을 해서 접근을 할 수 있도록 만들어주는 그런 항들이 또 보이게는 됩니다.

그렇지만 일단 기본적으로 모든 응용 프로그램들은 이 서치라는 디렉터리에 접속하는 것을 금지한다.
허용하지 않는다고 해놓고, 그런데 조금 전에 작성했던 이 파이썬 프로그램은 여기에 등록이 되어 있을 리는 만무하지요.
그러니까 그대로 이 규칙이 적용이 되는 것입니다.
접속을 하지 못한다.
그래서 Forbidden이라는 403 에러 코드를 받게 된 것입니다.

지금까지의 과정을 보시면 웹 사이트에 접속을 하기 전에 먼저 이 웹 사이트가 어떤 로봇 정책을 가지고 있는지 확인을 할 필요가 있지요.
그 확인하는 과정을 전체를 다 보여드릴 수는 없지만 최소한 이 웹 사이트가 가지고 있는 robots.txt 파일을 가져오는 과정만을 한번 보도록 하겠습니다.


**********************************실습영상***********************************
자, 소스 코드를 한 번 더 만들겠습니다.
이번에는 getGoogleRobotPolicy.py라는 제목으로서 코딩을 해보겠습니다.
코딩된 부분을 보시기 바랍니다.
원래는 이 robots.txt 파일을 제대로 해석을 하려면 이 urllib 패키지 안에 있는 robotparser라고 하는 모듈을 가져와야 됩니다.
가져왔습니다.
가져왔지만 이 소스 코드 자체에서는 그 기능을 사용하지는 않고, 단지 robots.txt 파일의 내용이 다운로드 되는 과정만 보도록 하겠습니다.

어디를 여는가?
조금 전에 보셨던 대로 구글이 가지고 있는 로봇 정책이 포함되어 있는 robots.txt 파일을 URL로 지정하겠습니다.
그리고 이 URL을 가지고 오픈을 시켜서 오픈돼서 나온 page 속에 들어있는 이 html 소스 코드 또는 robots.txt 파일 안에 있는 내용물이 무엇인지를 이 read 메소드를 통해서 보겠다는 것입니다.

물론 이 robots.txt 파일 안에 들어있는 그 내용물은 단순한 텍스트이지 다른 로봇 정책을 표현하는 policy를 표현하는 포맷을 가진 텍스트이기는 한데, html은 아니지요.
그 점만 유의를 해주시면 되겠습니다.
자, 실행을 해보겠습니다.
이와 같이 굉장히 많은 양의 데이터가 지금 dump가 됐습니다.
그 dump 되는 과정을 보면 TERMINAL 제가 잘못 눌러서 다른 데로 갔었는데 TERMINAL로 다시 되돌아왔습니다.
Robotpolicy.py를 실행을 시켰습니다.

그래서 구글의 robots.txt의 내용은 무엇인가.
User-agent부터 시작해서 이 xml까지 끝나는 이 부분까지가 이 robots.txt의 파일이 되는 것입니다.
그러고 나서 이 내용을 robot parser 모듈 속에 있는 여러 가지 함수 또는 클래스들로서 해석을 해내서 과연 내가 이 사이트에 접속할 수 있는지, 없는지를 판별해내는 것입니다.

*************************************끝**************************************

그런 과정을 위한 전 단계로서 다운로드 하는 과정만 코딩을 해서 확인을 해보았습니다.
그다음에는 조금 더 본격적으로 분석을 할 웹에 접속을 하도록 하겠습니다.
그리고 다운로드 받은 내용을 그대로 해석을 해서 콘텐츠에 대한 정보를 추출하고 그 정보를 CSV 파일까지 저장하는 과정을 보도록 하겠습니다.

자, 이번 과정에서는 단계 단계별로 여러분에게 작은 소스 코드들의 조각들을 제시하면서 설명을 드릴 것이기 때문에 한 덩어리의 큰 완전한 코드를 작성한 다음에 보여드리기에 편한 이 비주얼 스튜디오 코드보다는 주피터를 이용해서 다시 보여드리도록 하겠습니다.


**********************************실습영상***********************************
이제 아나콘에 들어가서 주피터를 실행시키겠습니다.


VSC 코드는 잠시 접어두겠습니다.
그리고 주피터를 실행시키고, 여기에 PythonProgramming 디렉터리로 들어가서 이번 차시에 해당하는 디렉터리로 또 들어가겠습니다.
WebDataHandlling 14차시입니다.
이 14차시의 WP로 들어가겠습니다.

여기에 조금 전에 만들어두었던 파이썬 프로그램 코드들이 보이고 있고요.
여기에서 이것이 중요한 것은 아니고, 여기에서 새로운 Python3의 노트북을 만들어보도록 하겠습니다.
그리고 이 노트북의 제목을 Untitled가 아니고, PP-14-WebDataHandlling이라는 이름을 붙인 노트북으로서 지정을 하겠습니다.
첫 번째 셀이 열렸습니다.

*************************************끝**************************************

지금부터 하고자 하는 것은 하나의 웹 사이트에 접속하고 그 웹 사이트가 페이지에 가지고 있는 html 소스 코드를 다운로드 받고, 이 다운로드 받은 소스 코드를 파징 즉 해석을 해서 그 안에 있는 여러 가지 콘텐츠들에 대한 정보를 따로 추출하고, 추출된 콘텐츠 정보들을 CSV 파일로 저장하는 과정까지 연속적으로 살펴보도록 하겠습니다.


**********************************실습영상***********************************
먼저 우리가 접속할 웹 사이트가 어떤 곳인지 한번 가보겠습니다.
웹 브라우저를 먼저 실행시켜보겠습니다.
이 웹 브라우저에서 우리가 가고자 하는 URL은 이 alansimpson이라는 부분은 이 사람의 이름인데요.
제가 이 강좌를 만들 때, 참조를 한 책의 저자가 만들어놓은 웹 사이트입니다.
이 웹 사이트에 들어가서 접속을 해서 분석을 하고자 하는 것입니다.
들어가 보겠습니다.

이런 내용입니다.
여러분 앞에 강의에서 보셨던 그 강의 노트에 있던 사이트의 그림이 그대로 나타나고 있지요.
소스 코드를 또 보겠습니다.
페이지 소스 코드를 보면 head 부분이 있고, body 부분이 나오고 있습니다.
그다음에 이 body 부분에서 조금 더 자세히 보시면 여기에 article이라는 부분이 있을까요?
article이 있습니다.
지금 article이 처음 시작이 있고, 그다음에 끝이 있는데 이 부분이 전부 anchor record들로 쭉 형성이 되어 있습니다.

앞에서 지금 보신 이 그림 하나하나가 anchor record 하나의 anchor record의 대응이 되고 있는 것을 보여드린 것입니다.
이와 같은 구조를 보면 여기에 a의 href, img, span 이런 3개의 구성요소로 되어 있는 anchor record가 있는 반면에 이 위에서 보시면 anchor record가 href, id, title 등으로만 구성되어 있는 경우도 있습니다.
이렇게 우리가 목적으로 하는 것은 이 부분은 아니지요.

이 위에 있는 부분이 아니고, 바로 이 href, img, span 이 3가지의 attribute가 있는 부분을 집중적으로 우리는 분석을 하려고 하는 것입니다.
이런 구조인 경우에는 끌어내지만 만일 이 구조와 다른 구조를 가진 부분들이 나온다면 무시를 할 필요도 있는 것이지요.
그런 의도를 담아서 소스를 코딩을 할 필요가 있는 것입니다.
이제 소스 코딩하는 부분으로 다시 돌아가겠습니다.
주피터로 돌아가겠습니다.
처음에 웹 접속을 해야 되겠지요.

그 웹 접속에 필요한 모듈이 urllib입니다.
import를 했습니다.
이렇게 코딩이 됐습니다.
조금 전에 브라우저를 통해서 보았던 웹 페이지의 URL이 그대로 여기 이 소스 코드에 사용되고 있습니다.
alansipson.me/python/cheatsheet/index.html을 URL로 지정을 했고, web_url이라는 이름으로 지정을 했지요.
그리고 request.urlopen의 파라미터로서 주었습니다.
인수로 주었지요.

접속한 결과로 얻은 그 페이지의 소스 코드는 다운로드를 받아서 이 페이지 객체 속으로 배정을 했습니다.
일단은 이 접속이 잘 됐는지 page.code, response의 code를 보고 판단하는 것까지를 1단계로 하겠습니다.
접속 확인 단계까지 하겠습니다.
run을 하겠습니다.
response code가 200이 나왔습니다.
즉 우리가 request를 했습니다.
urlopen을 통해서 request를 했고, 웹 사이트에서 response를 함으로써 그 주어진 결과를 페이지에 담았고, 그것이 제대로 된 내용으로서 response가 된 것인지 확인하기 위해서 코드를 찍어서 200이라는 것을 Okay라는 것을 확인한 것입니다.

그다음 단계는 무엇이지요?
이다음 단계에는 다운로드 받은 페이지에 들어있는 페이지 소스 코드를 파징을 하는 단계입니다.
파징을 하기 위해서는 필요한 것이 BeautifulSoup지요.
BeautifulSoup는 BS4 모듈 속에 들어있습니다.
bs4 import BeautifulSoup 그다음에 파징을 하는 우리가 필요한 부분을 추출하는 코딩을 해보겠습니다.

코드를 함께 보시겠습니다.
코딩이 됐습니다.
BeautifulSoup 클래스를 생성자로 해서 여기에 page를 넣었습니다.
이 page라는 것을 파징을 할 것이다.
page 속에 들어있는 html 코드 어디에서 들어왔다고요?
그 위의 셀을 보시면 바로 이 url로부터 들어온 html 코드지요.

그 코드를 파징을 하기 위한 해석을 하기 위해서 Soup라는 객체를 만들었고, 그 Soup라는 객체 속에 파징 대상이 되는 html 소스 코드까지 같이 집어넣은 상태가 된 것입니다.
그다음에 contents는 빈 list를 만들었습니다.
empty list를 만들었습니다.
그리고 for 루프를 만들었습니다.

*************************************끝**************************************

앞에서 이 사이트의 소스 코드를 보셨을 때, anchor record가 여러 개가 나와 있지요.
즉 타일 형태로 붙여진 그 그림들, 그 아이콘들의 콘텐츠를 표현하기 위해서 여러 개의 anchor record가 형성이 되어 있었는데 그런 것들을 순차적으로 되풀이해서 처리하기 위해서 for 루프를 만든 것입니다.

그리고 주로 여기에서는 그 anchor record를 발견하기 위해서 a라는 attribute 태그의 이름을 주었습니다.
a라는 태그 이름을 준 것이지요.
그런데 유의할 점은 body에서의 anchor record 하고, head에서의 anchor record가 구조가 달랐지요.
그리고 body에서의 anchor record에는 우리가 원하는 콘텐츠의 정보가 들어있었지만 head에 있는 anchor record 속에는 우리가 원하는 콘텐츠 정보가 들어있지는 않았습니다.
그런 anchor record는 무시를 해야 되지요.


**********************************실습영상***********************************
어느 경우에만 우리가 소스 코드를 추출해야 될까요?
바로 url도 있고, 즉 href 속성도 있고, src 속성도 있고 그다음에 span이라는 태그도 있는 이런 형태의, 이런 3가지의 구성요소가 완전히 되어 있는 경우에만 추출을 해서 이 contents라는 list에 append를 시켜서 모아놓아야 되는 것이지요.
만일 그렇지 않다면 무엇인가 except가 발생합니다.
img를 찾았는데 img가 없다.
span을 찾았는데 span이 없다.
url은 아마 대부분 다 있을 것입니다.
이런 경우에는 다 attribute 에러가 발생합니다.

찾아야 하는 attribute가 실제는 없다.
이럴 때는 exception이 발생해서 이 except 부분으로 넘어오게 되는데 여기에서 하는 일은 아무 것도 없습니다.
그냥 무시하고 넘어가서 다음번을 찾아라.
이 for 루프로 다시 되돌아가서 다음 anchor record를 찾으라는 것을 지시를 받으면 되는 것입니다.
결과로 봤을 때는 우리가 원하는 콘텐츠를 담은 anchor record만 이 contents list에 담겨서 나오게 되는 것이지요.

그런 의도로서 작성한 코드가 되는 것입니다.
이런 코드가 동작을 하면 어떤 결과가 나올지 보겠습니다.
자, 이제 우리는 이 contents가 가지고 있는 레코드의 내용을 전부 Display를 해보는 것으로서 그 결과를 확인하도록 하겠습니다.
run을 시켜보겠습니다.
자, 여기에서 우리가 예상하지 못했던 에러가 발생했는데요.
이 에러는 BeautifulSoup에서는, 보니까 지금 오류를 발견했습니다.

BeautifulSoup가 왜 없나, 하고 찾아봤더니 스펠은 맞지만 대소문자가 틀렸네요.
대문자를 써야 되는데, 소문자를 쓴 것이 잘못이었습니다.
다시 run을 시키겠습니다.
이제 제대로 실행이 돼서 contents가 형성이 됐습니다.
즉 contents에 대한 정보를 가진 a 태그 record들의 list가 형성이 돼서 지금 Display가 된 것입니다.
그 내용 하나를 보시면 list가 대괄호로 시작되고 있습니다.
그리고 첫 번째 Dictionary입니다.

url이라는 key에 대해서 그 value는 anacondastarter 이 img라고 하는 key에 대해서는 anacondastarter 슬래시 anacondastarter256.jpg 그다음에 span이라는 key에 대해서는 anacondastarter라는 string 여기까지가 하나의 Dictionary를 구성하고 있는 것이지요.
이런 구조의 Dictionary가 몇 개가 지금 형성이 되어 있느냐 하면 몇 개인지는 지금 헤아리기가 어렵지요.
여러 개가 지금 형성이 되어 있습니다.

*************************************끝**************************************

그리고 이런 하나하나의 Dictionary들은 무엇에 대응하는가 하면 소스 코드에서 보시면 하나의 a태그 레코드에 대응이 되는 것입니다.
그 안에 있는 href 또 img 그다음에 span이라는 3개의 attribute 내지는 태그에 대한 항목들을 가지고 있는 Dictionary가 되는 것입니다.


**********************************실습영상***********************************
이런 Dictionary가 여러 개가 모여서 하나의 전체적인 list를 구성하는 것입니다.
여기까지 확인을 했습니다.
즉 다운로드 받은 HTML 소스로부터 우리가 원하는 정보를 가진 부분을 추출했다는 것까지 본 것입니다.

*************************************끝**************************************

자, 다음 단계는 그러면 무엇일까요?
바로 이 정보를 우리가 원하는 포맷으로 저장을 하는 것이 마지막 단계가 되는 것이지요.
그 마지막 단계를 향해서 또 코딩에 들어가도록 하겠습니다.


**********************************실습영상***********************************
이제 새로운 셀에서 보겠습니다.
자, 코딩이 됐습니다.
그 코딩 된 부분을 보겠습니다.
파일로 저장을 할 때, 사용하는 파일 포맷은 바로 CSV입니다.
Comma-Separated Values 포맷으로 save를 하기 위해서 거기에 관련된 모듈 csv를 import를 했습니다.
그리고 csv 파일을 오픈했는데 라이트 모드로 오픈을 했습니다.

newline은 nullstring을 집어넣었습니다.
즉 newline은 라이트 할 때는 생기지 않도록 한다는 것입니다.
그러면 어느 때 생길까요?
바로 이미 newline은 다른 곳에서 만들어지는 것이지요.
여기에서 newline이 만들어지면 중복이 되어버리기 때문에 여기에서는 생기지 않도록 일단은 막아놓은 것입니다.

그리고 이 파일에 대한 객체를 csvf라고 이름을 지었습니다.
그다음에 csvwriter 객체를 만들어야 되겠지요.
즉 CSV 파일을 Handling하고 쓸 수 있는 객체를 만드는 것입니다.
그것은 csv.writer라는 함수를 써서 csvf를 인수로 주어서 이 csvf에 대한 writer를 만드는 것입니다.
만들었고, 이제 첫 번째 header title row를 저장하겠습니다.

*************************************끝**************************************

우리가 추출한 a 레코드에 들어있는 속성의 종류는 3가지입니다.
url 또 img 그다음에 span 텍스트입니다.
그런 3가지 속성의 데이터를 각각 한 column씩 지정을 해서 Save를 하기 위해서 각 column에 타이틀을 주는데 단순하게 url, img, span이라는 세 단어들로 지정해서 넣도록 하겠습니다.


**********************************실습영상***********************************
writerow를 하면 이제 파일에 들어가게 됩니다.
한 줄이 생성이 되는데 한 row가 생성이 되는 것이지요.
거기에 이 url, img, span이 콤마로 separate 돼서 들어있는 형태의 텍스트 파일이 되는 것입니다.
그리고 나머지 이 부분들 바로 이 콘텐츠에 해당하는 여러 개의 Dictionary 형태의 객체로 저장되어 있는 이 데이터는 그러면 어떻게 저장을 할 것인가.
이 저장을 하는 것도 굉장히 단순합니다.

단지 이 하나하나의 Dictionary를 row로 받았지요.
for 루프에서는 contents라는 list에서 항목을 row로 받는다면 이 row는 어떤 유형이 될까요?
바로 Dictionary 유형이 되는 것이지요.
어떤 Dictionary?
이 list의 한 항목이 되는 Dictionary가 되는 것입니다.
여기까지의 url부터 span까지 해당하는 한 3개의 항목을 가진 Dictionary가 되고, 그 Dictionary에서 각각의 key를 주어서 각각의 항목값을 빼내는 것입니다.

url이라는 key에 대해서는 이런 anacondastarter라는 url이 나오는 것이고, row img라는 key에 대해서는 이 img key에 해당하는 항목값이 나오게 되는 것이지요.
anacondastarter 256.jpg라는 string들이 나오고 span도 마찬가지입니다.
이런 3개의 string들을 각각 한 줄씩 써나가는 것입니다.
그리고 각각의 string들의 사이사이에는 콤마가 들어가게 되는 형태로 쓰이게 되는 것이지요.

이런 형태로 해서 한 줄 한 줄 이 for 루프를 돌면서 써넣으면 바로 csv 파일이 형성되는 것입니다.
그 csv 파일이 형성되는 과정을 실행시켜보겠습니다.
run을 해보겠습니다.
Program done이 됐습니다.
별다른 내용이 화면에 나타나지는 않았는데, 이때 이 코드를 실행시킴으로써 무엇이 형성됐느냐 하면 이 디스크 안에는 바로 csvfile.csv 파일이 형성이 된 것입니다.
그 파일을 한번 쫓아가보겠습니다.

어디에서 보면 될까요?
파일 탐색기를 우선 열면 C 드라이브에서 사용자 hycu 그다음에 Python programming 그다음에 14번 WP에서 생겼지요.
csvfile.csv라는 형태로 생겼습니다.
이것을 먼저 노트 패드로 한번 열어보겠습니다.
메모장이지요.
메모장.
어떻게 되어 있나요?

가장 첫 번째 줄은 first로 url 또 img 그다음에 span이라는 3개의 단어가 콤마를 사이사이에 둔 형태로 separate 돼서 들어있습니다.
그다음에는 anacondastarter, anacondastarter/, anacondastarter256.jpg 그다음에 anacondastarter라는 string이 역시 csv 포맷으로 쭉 들어있습니다.
이렇게 텍스트 에디터로 봐서는 이게 무슨 내용인지 참 구별하기가 어렵고, 처리하기도 어렵지요.

*************************************끝**************************************

이제는 이 csv 파일을 만든 가장 큰 이유 중에 하나는 우리가 흔히 편하게 사용할 수 있는 데이터 처리 도구인 엑셀을 사용할 수 있도록 하기 위해서 이런 유형의 파일로 저장을 한 것이지요.


**********************************실습영상***********************************
엑셀을 통해서 열어보도록 하겠습니다.
연결 프로그램에 엑셀이 있지요.
엑셀로 열렸는데요.
지금 이 형태도 너무 셀들이 폭이 좁아서 보기가 힘들지요.
이것을 적당하게 모양을 잡아주면 이와 같이 지금 이 부분은 하나의 헤더 타이틀입니다.

헤더 타이틀이고, 이것을 이렇게 중심 센트릭으로 표시가 되도록 한다면 url들은 이런 것들이고, 여기에 함께 있는 이미지들은 이런 디렉터리에 들어있고, 그다음에 span 텍스트들은 이런 내용으로 설명이 붙어 있다고 정리를 해서 이 엑셀 파일로 만들어낼 수 있는 것입니다.
그리고 이 엑셀 파일을 통해서 여러 가지 데이터 처리 과정을 또 밟을 수 있는 것이지요.

*************************************끝**************************************

이번 차시에는 이렇게 특정한 웹 사이트에 들어가서 그 웹 사이트로부터 소스 코드를 다운로드 받고, 그 소스 코드 중에서 우리가 원하는 콘텐츠가 어디 있는지 대충 타깃을 범위를 정한 다음에 그 부분을 집중적으로 또 분석해서 데이터를 꺼내고 즉 콘텐츠에 대한 위치 정보들이지요.
지금 다룬 내용들로 봐서는.

그런 위치 정보들을 다시 다른 응용 프로그램으로 처리하기 편하도록 특정한 포맷으로 저장하는 과정까지 우리가 실행을 해본 것입니다.
자, 이번 차시를 끝으로 긴 여정을 끝내게 됐습니다.
그동안 이 파이썬 프로그램의 아주 시초부터 또 어느 정도는 여러분이 응용할 수 있는 중급 단계까지 다다르도록 많은 시간 동안 함께 공부하시면서 수고를 많이 해주셨습니다.
감사합니다.
